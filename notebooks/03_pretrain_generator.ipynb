{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e60e91e",
   "metadata": {},
   "source": [
    "# Loading molecules from Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc664eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, encoded_path, properties_csv):\n",
    "        \"\"\"\n",
    "        encoded_path: path to train_encoded.pt (tensor of token IDs)\n",
    "        properties_csv: path to CSV with normalized properties\n",
    "        \"\"\"\n",
    "        # Load encoded token sequences\n",
    "        self.encoded_sequences = torch.load(encoded_path)  # shape: [num_molecules, seq_len]\n",
    "\n",
    "        # Load property vectors\n",
    "        self.props_df = pd.read_csv(properties_csv)\n",
    "        # Only keep the property columns you want\n",
    "        self.prop_columns = ['QED', 'SAS', 'LogP', 'TPSA', 'MolWt']\n",
    "        self.properties = torch.tensor(\n",
    "            self.props_df[self.prop_columns].values,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        # Sanity check: number of sequences and properties must match\n",
    "        assert len(self.encoded_sequences) == len(self.properties), \\\n",
    "            \"Mismatch: sequences vs properties\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            seq_tensor: LongTensor of token IDs (padded)\n",
    "            prop_tensor: FloatTensor of property values\n",
    "        \"\"\"\n",
    "        seq_tensor = self.encoded_sequences[idx]\n",
    "        prop_tensor = self.properties[idx]\n",
    "        return seq_tensor, prop_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b57624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_8000\\4151915843.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.encoded_sequences = torch.load(encoded_path)  # shape: [num_molecules, seq_len]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([ 1, 51, 11, 51, 51, 51,  4,  7, 51, 12, 51, 61, 51, 13, 61, 51, 51, 61,\n",
      "        51, 13, 61, 12,  5, 51, 51, 11, 69,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0])\n",
      "Properties: tensor([0.6461, 0.1320, 0.6085, 0.0123, 0.0110])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MoleculeDataset(\n",
    "    encoded_path=\"../data/processed_5l/train_encoded.pt\",\n",
    "    properties_csv=\"../data/processed_5l/train_properties.csv\"\n",
    ")\n",
    "\n",
    "# Example: inspect first sample\n",
    "seq, prop = dataset[0]\n",
    "print(\"Token IDs:\", seq)\n",
    "print(\"Properties:\", prop)\n",
    "\n",
    "# DataLoader for batch training\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Example: one batch\n",
    "for batch_seq, batch_props in dataloader:\n",
    "    print(batch_seq.shape)   # [64, seq_len]\n",
    "    print(batch_props.shape) # [64, 5]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668d0223",
   "metadata": {},
   "source": [
    "# Conditional + unconditional Generator Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37325803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, prop_dim, d_model=256, nhead=8, num_layers=4, max_len=128, dropout=0.1): \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.prop_embed = nn.Linear(prop_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=512,\n",
    "            batch_first=False,\n",
    "            dropout=dropout  \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, props):\n",
    "        src = torch.clamp(src, 0, self.token_embed.num_embeddings - 1)\n",
    "        B, L = src.shape\n",
    "        tok_emb = self.token_embed(src) * (self.d_model ** 0.5)\n",
    "        pos = torch.arange(L, device=src.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embed(pos)\n",
    "        prop_emb = self.prop_embed(props).unsqueeze(1)\n",
    "        \n",
    "        x = tok_emb + pos_emb + prop_emb\n",
    "        x = self.dropout(x) \n",
    "        x = x.transpose(0, 1)  # [seq_len, batch, dim]\n",
    "        \n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(src.device)\n",
    "        out = self.transformer(x, mask=tgt_mask)\n",
    "        \n",
    "        out = out.transpose(0, 1) # Back to [batch, seq_len, dim]\n",
    "        logits = self.fc_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd5193",
   "metadata": {},
   "source": [
    "# Training 20 epoch trained generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 70\n",
      "Using: cuda\n",
      "‚ö†Ô∏è No checkpoint found. Starting training from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:42<00:00, 15.43it/s, avg_loss=1.1947]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:49<00:00, 15.04it/s, avg_loss=0.9701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:49<00:00, 15.06it/s, avg_loss=0.9097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_3.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:46<00:00, 15.21it/s, avg_loss=0.8737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:57<00:00, 14.67it/s, avg_loss=0.8481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:51<00:00, 14.95it/s, avg_loss=0.8289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_6.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.18it/s, avg_loss=0.8136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.17it/s, avg_loss=0.8010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:49<00:00, 15.03it/s, avg_loss=0.7899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_9.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:43<00:00, 15.36it/s, avg_loss=0.7806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.25it/s, avg_loss=0.7723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.28it/s, avg_loss=0.7647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_12.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:46<00:00, 15.21it/s, avg_loss=0.7567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:44<00:00, 15.30it/s, avg_loss=0.7502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.25it/s, avg_loss=0.7431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_15.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:44<00:00, 15.34it/s, avg_loss=0.7369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:43<00:00, 15.37it/s, avg_loss=0.7314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:43<00:00, 15.39it/s, avg_loss=0.7253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_18.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:43<00:00, 15.36it/s, avg_loss=0.7206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:43<00:00, 15.39it/s, avg_loss=0.7162]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch, gc, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "vocab_size = int(torch.max(dataset.encoded_sequences)) + 1\n",
    "print(\"Vocab size =\", vocab_size)\n",
    "\n",
    "dataset.encoded_sequences = dataset.encoded_sequences.long()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "model = Generator(vocab_size=vocab_size, prop_dim=5, max_len=128, dropout=0.1).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # assuming 0 is padding token\n",
    "\n",
    "start_epoch = 0 \n",
    "\n",
    "p_uncond = 0.1 \n",
    "TOTAL_EPOCHS = 20 \n",
    "\n",
    "if torch.max(dataset.encoded_sequences) >= vocab_size:\n",
    "    print(\" Token index out of range! Check vocab size.\")\n",
    "\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    batch_iterator = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        # --- Update description to show correct epoch numbers ---\n",
    "        desc=f\"Epoch {epoch+1}/{TOTAL_EPOCHS}\", \n",
    "        total=len(dataloader)\n",
    "    )\n",
    "\n",
    "    for i, (seqs, props) in batch_iterator:\n",
    "        seqs, props = seqs.to(device), props.to(device)\n",
    "\n",
    "        if torch.rand(1).item() < p_uncond:\n",
    "            props = torch.zeros_like(props)\n",
    "\n",
    "        inputs = seqs[:, :-1]\n",
    "        targets = seqs[:, 1:]\n",
    "        \n",
    "        max_token = torch.max(seqs)\n",
    "        if max_token >= vocab_size:\n",
    "            print(f\" Token {max_token} >= vocab_size {vocab_size}\")\n",
    "            raise ValueError(\"Token index out of range!\")\n",
    "\n",
    "        logits = model(inputs, props) \n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        running_avg_loss = total_loss / (i + 1)\n",
    "        batch_iterator.set_postfix(avg_loss=f\"{running_avg_loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "    print() \n",
    "\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        checkpoint_path = f\"../results/models_5l/u&c_generator_epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"--- Checkpoint saved to {checkpoint_path} ---\")\n",
    "\n",
    "print(\" Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a64797",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=\"../results/models_5l/u&c_generator_epoch_20.pt\"\n",
    "torch.save({\n",
    "            'epoch': 20,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f91ce",
   "metadata": {},
   "source": [
    "# Testing 20 epoch trained generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model from epoch 20 with loss 0.7162\n",
      "üß¨ Generating with conditions: tensor([[0.6461, 0.1320, 0.6085, 0.0123, 0.0110]], device='cuda:0')\n",
      "Generated token sequence:\n",
      " [1, 24, 24, 4, 24, 5, 51, 11, 51, 4, 24, 4, 20, 35, 5, 35, 5, 51, 51, 12, 51, 4, 24, 59, 5, 51, 51, 4, 24, 59, 5, 51, 4, 24, 59, 5, 51, 12, 51, 11, 24, 59, 69]\n",
      "üß¨ Decoded SMILES: CC(C)c1c(C(=O)O)cc2c(Cl)cc(Cl)c(Cl)c2c1Cl<END>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = 70\n",
    "prop_dim = 5\n",
    "max_len_model = 128\n",
    "\n",
    "model = Generator(vocab_size, prop_dim, max_len=max_len_model,dropout=0.1).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"../results/models_5l/u&c_generator_epoch_20.pt\", map_location=device,weights_only=True) \n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\" Loaded model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "test_props = torch.tensor([[0.6460672474469629,0.13202386890910442,0.6084964566032581,0.012271807687701365,0.011000075433875664]], dtype=torch.float32).to(device)\n",
    "print(\" Generating with conditions:\", test_props)\n",
    "\n",
    "\n",
    "props_to_use = test_props \n",
    "\n",
    "start_token_id = 1  \n",
    "stop_token_id = 69   \n",
    "max_gen_len = 128\n",
    "top_k = 50\n",
    "\n",
    "generated = torch.tensor([[start_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_gen_len):\n",
    "        \n",
    "        logits = model(generated, props_to_use)\n",
    "        last_logits = logits[:, -1, :]\n",
    "\n",
    "        v, _ = torch.topk(last_logits, top_k)\n",
    "        last_logits[last_logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        if next_token.item() == stop_token_id:\n",
    "            break\n",
    "\n",
    "print(\"Generated token sequence:\\n\", generated.cpu().numpy().tolist()[0])\n",
    "\n",
    "token_to_idx = {\n",
    "    \"#\": 2, \"%\": 3, \"(\": 4, \")\": 5, \"+\": 6, \"-\": 7, \".\": 8, \"/\": 9, \"0\": 10, \"1\": 11, \"2\": 12, \"3\": 13,\n",
    "    \"4\": 14, \"5\": 15, \"6\": 16, \"7\": 17, \"8\": 18, \"9\": 19, \"=\": 20, \"@\": 21, \"A\": 22, \"B\": 23, \"C\": 24,\n",
    "    \"D\": 25, \"E\": 26, \"F\": 27, \"G\": 28, \"H\": 29, \"I\": 30, \"K\": 31, \"L\": 32, \"M\": 33, \"N\": 34, \"O\": 35,\n",
    "    \"P\": 36, \"R\": 37, \"S\": 38, \"T\": 39, \"U\": 40, \"V\": 41, \"W\": 42, \"X\": 43, \"Y\": 44, \"Z\": 45, \"[\": 46,\n",
    "    \"\\\\\": 47, \"]\": 48, \"a\": 49, \"b\": 50, \"c\": 51, \"d\": 52, \"e\": 53, \"f\": 54, \"g\": 55, \"h\": 56, \"i\": 57,\n",
    "    \"k\": 58, \"l\": 59, \"m\": 60, \"n\": 61, \"o\": 62, \"p\": 63, \"r\": 64, \"s\": 65, \"t\": 66, \"u\": 67,\n",
    "    \"y\": 68, \"<PAD>\": 0, \"<START>\": 1, \"<END>\": 69}\n",
    "\n",
    "idx_to_token = {v: k for k, v in token_to_idx.items()}\n",
    "generated_seq = generated.cpu().numpy().tolist()[0]\n",
    "decoded = ''.join(idx_to_token.get(tok, '?') for tok in generated_seq if tok not in [0, 1, 68])\n",
    "print(\" Decoded SMILES:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16965fb3",
   "metadata": {},
   "source": [
    "# Training for 50 epochas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848737e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 70\n",
      "Using: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Resuming training from epoch 21. Last loss was 0.7162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:55<00:00, 14.73it/s, avg_loss=0.7124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_21.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [05:18<00:00, 13.67it/s, avg_loss=0.7077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [05:19<00:00, 13.66it/s, avg_loss=0.7045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:52<00:00, 14.89it/s, avg_loss=0.7015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_24.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:52<00:00, 14.90it/s, avg_loss=0.6974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:53<00:00, 14.85it/s, avg_loss=0.6940]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:55<00:00, 14.75it/s, avg_loss=0.6924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_27.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:49<00:00, 15.08it/s, avg_loss=0.6890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.15it/s, avg_loss=0.6865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.24it/s, avg_loss=0.6839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_30.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.26it/s, avg_loss=0.6817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.24it/s, avg_loss=0.6789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.14it/s, avg_loss=0.6773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_33.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:52<00:00, 14.91it/s, avg_loss=0.6745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:52<00:00, 14.92it/s, avg_loss=0.6725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:49<00:00, 15.07it/s, avg_loss=0.6708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_36.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:46<00:00, 15.21it/s, avg_loss=0.6687]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.16it/s, avg_loss=0.6674]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:48<00:00, 15.13it/s, avg_loss=0.6653]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_39.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.17it/s, avg_loss=0.6632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.18it/s, avg_loss=0.6623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:46<00:00, 15.19it/s, avg_loss=0.6604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_42.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:47<00:00, 15.19it/s, avg_loss=0.6589]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:46<00:00, 15.22it/s, avg_loss=0.6574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:50<00:00, 14.99it/s, avg_loss=0.6563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_45.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:49<00:00, 15.04it/s, avg_loss=0.6546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:48<00:00, 15.09it/s, avg_loss=0.6521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.24it/s, avg_loss=0.6513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/u&c_generator_epoch_48.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.25it/s, avg_loss=0.6481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4359/4359 [04:45<00:00, 15.25it/s, avg_loss=0.6450]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch, gc, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "vocab_size = int(torch.max(dataset.encoded_sequences)) + 1\n",
    "print(\"Vocab size =\", vocab_size)\n",
    "\n",
    "dataset.encoded_sequences = dataset.encoded_sequences.long()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\", device)\n",
    "\n",
    "model = Generator(vocab_size=vocab_size, prop_dim=5, max_len=128, dropout=0.1).to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  \n",
    "\n",
    "\n",
    "checkpoint_to_load = \"../results/models_5l/u&c_generator_epoch_20.pt\"\n",
    "start_epoch = 0\n",
    "\n",
    "if os.path.exists(checkpoint_to_load):\n",
    "    checkpoint = torch.load(checkpoint_to_load, map_location=device, weights_only=True)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    last_loss = checkpoint['loss']\n",
    "    \n",
    "    print(f\" Resuming training from epoch {start_epoch + 1}. Last loss was {last_loss:.4f}\")\n",
    "else:\n",
    "    print(\" No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "p_uncond = 0.1 \n",
    "TOTAL_EPOCHS = 50\n",
    "\n",
    "if torch.max(dataset.encoded_sequences) >= vocab_size:\n",
    "    print(\" Token index out of range! Check vocab size.\")\n",
    "\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    batch_iterator = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        desc=f\"Epoch {epoch+1}/{TOTAL_EPOCHS}\", \n",
    "        total=len(dataloader)\n",
    "    )\n",
    "\n",
    "    for i, (seqs, props) in batch_iterator:\n",
    "        seqs, props = seqs.to(device), props.to(device)\n",
    "\n",
    "        if torch.rand(1).item() < p_uncond:\n",
    "            props = torch.zeros_like(props)\n",
    "\n",
    "        inputs = seqs[:, :-1]\n",
    "        targets = seqs[:, 1:]\n",
    "        \n",
    "        max_token = torch.max(seqs)\n",
    "        if max_token >= vocab_size:\n",
    "            print(f\" Token {max_token} >= vocab_size {vocab_size}\")\n",
    "            raise ValueError(\"Token index out of range!\")\n",
    "\n",
    "        logits = model(inputs, props) \n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        running_avg_loss = total_loss / (i + 1)\n",
    "        batch_iterator.set_postfix(avg_loss=f\"{running_avg_loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "    print()\n",
    "\n",
    "    # save checkpoint every 3 epochs\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        checkpoint_path = f\"../results/models_5l/u&c_generator_epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"--- Checkpoint saved to {checkpoint_path} ---\")\n",
    "\n",
    "print(\" Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8bc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=\"../results/models_5l/u&c_generator_epoch_50.pt\"\n",
    "torch.save({\n",
    "            'epoch': 50,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb088c",
   "metadata": {},
   "source": [
    "# Testing 50 epoch trained generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd17cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model from epoch 50 with loss 0.6450\n",
      "üß¨ Generating with conditions: tensor([[0.6461, 0.1320, 0.6085, 0.0123, 0.0110]], device='cuda:0')\n",
      "Generated token sequence:\n",
      " [1, 38, 20, 38, 4, 20, 35, 5, 4, 35, 5, 34, 51, 11, 51, 51, 51, 4, 34, 24, 51, 12, 51, 51, 51, 51, 51, 12, 5, 51, 51, 11, 69]\n",
      "üß¨ Decoded SMILES: S=S(=O)(O)Nc1ccc(NCc2ccccc2)cc1<END>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = 70\n",
    "prop_dim = 5\n",
    "max_len_model = 128\n",
    "\n",
    "model = Generator(vocab_size, prop_dim, max_len=max_len_model,dropout=0.1).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"../results/models_5l/u&c_generator_epoch_50.pt\", map_location=device,weights_only=True) \n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "print(f\" Loaded model from epoch {checkpoint['epoch']} with loss {checkpoint['loss']:.4f}\")\n",
    "model.eval()\n",
    "\n",
    "test_props = torch.tensor([[0.6460672474469629,0.13202386890910442,0.6084964566032581,0.012271807687701365,0.011000075433875664]], dtype=torch.float32).to(device)\n",
    "print(\" Generating with conditions:\", test_props)\n",
    "\n",
    "props_to_use = test_props \n",
    "\n",
    "start_token_id = 1  # <START> token ID\n",
    "stop_token_id = 69   # <END> token ID\n",
    "max_gen_len = 128\n",
    "top_k = 50\n",
    "\n",
    "generated = torch.tensor([[start_token_id]], dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_gen_len):\n",
    "        # Model only needs to see the current sequence and target properties\n",
    "        logits = model(generated, props_to_use)\n",
    "        \n",
    "        # Get logits for the *last* token only\n",
    "        last_logits = logits[:, -1, :] # Shape: [batch, vocab_size]\n",
    "\n",
    "        v, _ = torch.topk(last_logits, top_k)\n",
    "        last_logits[last_logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # 3. Append the new token\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        # 4. Stop if we hit the <END> token\n",
    "        if next_token.item() == stop_token_id: \n",
    "            break\n",
    "\n",
    "print(\"Generated token sequence:\\n\", generated.cpu().numpy().tolist()[0])\n",
    "\n",
    "token_to_idx = {\n",
    "    \"#\": 2, \"%\": 3, \"(\": 4, \")\": 5, \"+\": 6, \"-\": 7, \".\": 8, \"/\": 9, \"0\": 10, \"1\": 11, \"2\": 12, \"3\": 13,\n",
    "    \"4\": 14, \"5\": 15, \"6\": 16, \"7\": 17, \"8\": 18, \"9\": 19, \"=\": 20, \"@\": 21, \"A\": 22, \"B\": 23, \"C\": 24,\n",
    "    \"D\": 25, \"E\": 26, \"F\": 27, \"G\": 28, \"H\": 29, \"I\": 30, \"K\": 31, \"L\": 32, \"M\": 33, \"N\": 34, \"O\": 35,\n",
    "    \"P\": 36, \"R\": 37, \"S\": 38, \"T\": 39, \"U\": 40, \"V\": 41, \"W\": 42, \"X\": 43, \"Y\": 44, \"Z\": 45, \"[\": 46,\n",
    "    \"\\\\\": 47, \"]\": 48, \"a\": 49, \"b\": 50, \"c\": 51, \"d\": 52, \"e\": 53, \"f\": 54, \"g\": 55, \"h\": 56, \"i\": 57,\n",
    "    \"k\": 58, \"l\": 59, \"m\": 60, \"n\": 61, \"o\": 62, \"p\": 63, \"r\": 64, \"s\": 65, \"t\": 66, \"u\": 67,\n",
    "    \"y\": 68, \"<PAD>\": 0, \"<START>\": 1, \"<END>\": 69}\n",
    "\n",
    "idx_to_token = {v: k for k, v in token_to_idx.items()}\n",
    "generated_seq = generated.cpu().numpy().tolist()[0]\n",
    "decoded = ''.join(idx_to_token.get(tok, '?') for tok in generated_seq if tok not in [0, 1, 68])\n",
    "print(\"üß¨ Decoded SMILES:\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
