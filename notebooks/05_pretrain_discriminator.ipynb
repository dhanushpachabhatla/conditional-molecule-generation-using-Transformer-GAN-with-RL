{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ede2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, gc\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fb6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Generator Class ---\n",
    "# We need this to load your pre-trained model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, prop_dim, d_model=256, nhead=8, num_layers=4, max_len=128, dropout=0.1): \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.prop_embed = nn.Linear(prop_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=512, batch_first=False, dropout=dropout  \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, props):\n",
    "        src = torch.clamp(src, 0, self.token_embed.num_embeddings - 1)\n",
    "        B, L = src.shape\n",
    "        tok_emb = self.token_embed(src) * (self.d_model ** 0.5)\n",
    "        pos = torch.arange(L, device=src.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embed(pos)\n",
    "        prop_emb = self.prop_embed(props).unsqueeze(1)\n",
    "        \n",
    "        x = tok_emb + pos_emb + prop_emb\n",
    "        x = self.dropout(x) \n",
    "        x = x.transpose(0, 1)  # Transformer expects [L, B, D]\n",
    "        \n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(src.device)\n",
    "        out = self.transformer(x, mask=tgt_mask)\n",
    "        \n",
    "        out = out.transpose(0, 1) \n",
    "        logits = self.fc_out(out)\n",
    "        return logits\n",
    "\n",
    "# --- 2. Discriminator Class (NEW) ---\n",
    "# This is an \"Encoder\" as you planned: it's a Transformer Encoder *without* a mask\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, prop_dim, d_model=256, nhead=8, num_layers=4, max_len=128, dropout=0.1): \n",
    "        super().__init__() # <-- FIX: __init__\n",
    "        self.d_model = d_model\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "        self.prop_embed = nn.Linear(prop_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=512, batch_first=False, dropout=dropout  \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, 1) # Output 1 logit for (Real/Fake)\n",
    "\n",
    "    def forward(self, src, props):\n",
    "        src = torch.clamp(src, 0, self.token_embed.num_embeddings - 1)\n",
    "        B, L = src.shape\n",
    "        \n",
    "        # This logic exactly matches your Generator's input prep\n",
    "        tok_emb = self.token_embed(src) * (self.d_model ** 0.5)\n",
    "        pos = torch.arange(L, device=src.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embed(pos)\n",
    "        prop_emb = self.prop_embed(props).unsqueeze(1)\n",
    "        \n",
    "        x = tok_emb + pos_emb + prop_emb\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1) # [L, B, D]\n",
    "        \n",
    "        # --- CRITICAL DIFFERENCE ---\n",
    "        # No mask! The Discriminator sees the *entire* sequence to judge it.\n",
    "        out = self.transformer(x) # Shape: [L, B, D]\n",
    "        \n",
    "        # Global Pooling: We use the embedding of the first token (<START>)\n",
    "        # as a [CLS] token to represent the entire sequence.\n",
    "        pooled_output = out[0, :, :] # Shape: [B, D]\n",
    "        \n",
    "        logit = self.fc_out(pooled_output) \n",
    "        return logit.squeeze(-1) # Output shape: [batch]\n",
    "\n",
    "# --- 3. Token Maps ---\n",
    "def get_token_maps():\n",
    "    token_to_idx = {\n",
    "    \"#\": 2, \"%\": 3, \"(\": 4, \")\": 5, \"+\": 6, \"-\": 7, \".\": 8, \"/\": 9, \"0\": 10, \"1\": 11, \"2\": 12, \"3\": 13,\n",
    "    \"4\": 14, \"5\": 15, \"6\": 16, \"7\": 17, \"8\": 18, \"9\": 19, \"=\": 20, \"@\": 21, \"A\": 22, \"B\": 23, \"C\": 24,\n",
    "    \"D\": 25, \"E\": 26, \"F\": 27, \"G\": 28, \"H\": 29, \"I\": 30, \"K\": 31, \"L\": 32, \"M\": 33, \"N\": 34, \"O\": 35,\n",
    "    \"P\": 36, \"R\": 37, \"S\": 38, \"T\": 39, \"U\": 40, \"V\": 41, \"W\": 42, \"X\": 43, \"Y\": 44, \"Z\": 45, \"[\": 46,\n",
    "    \"\\\\\": 47, \"]\": 48, \"a\": 49, \"b\": 50, \"c\": 51, \"d\": 52, \"e\": 53, \"f\": 54, \"g\": 55, \"h\": 56, \"i\": 57,\n",
    "    \"k\": 58, \"l\": 59, \"m\": 60, \"n\": 61, \"o\": 62, \"p\": 63, \"r\": 64, \"s\": 65, \"t\": 66, \"u\": 67,\n",
    "    \"y\": 68, \"<PAD>\": 0, \"<START>\": 1, \"<END>\": 69}\n",
    "    idx_to_token = {v: k for k, v in token_to_idx.items()}\n",
    "    return token_to_idx, idx_to_token\n",
    "\n",
    "# --- 4. Dataset---\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, encoded_path, properties_csv):\n",
    "        super().__init__() # <-- FIX: __init__\n",
    "        \n",
    "        # Load property vectors\n",
    "        self.props_df = pd.read_csv(properties_csv)\n",
    "        self.prop_columns = ['QED', 'SAS', 'LogP', 'TPSA', 'MolWt']\n",
    "        self.properties = torch.tensor(\n",
    "            self.props_df[self.prop_columns].values,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "\n",
    "        # Load encoded token sequences\n",
    "        print(f\"Loading real encoded sequences from {encoded_path}...\")\n",
    "        self.encoded_sequences = torch.load(encoded_path, weights_only=True)\n",
    "        print(\"✅ Loaded encoded sequences.\")\n",
    "        \n",
    "        assert len(self.encoded_sequences) == len(self.properties), \"Mismatch: sequences vs properties\"\n",
    "\n",
    "    def __len__(self): # <-- FIX: __len__\n",
    "        return len(self.encoded_sequences)\n",
    "\n",
    "    def __getitem__(self, idx): # <-- FIX: __getitem__\n",
    "        seq_tensor = self.encoded_sequences[idx]\n",
    "        prop_tensor = self.properties[idx]\n",
    "        return seq_tensor, prop_tensor\n",
    "\n",
    "# --- 5. Correct Fake Sample Generator ---\n",
    "# This generates *full, autoregressive* samples\n",
    "def generate_fake_samples(generator, props_to_use, batch_size, device, token_maps, max_len=128):\n",
    "    generator.eval() # Generator is for inference here\n",
    "    \n",
    "    token_to_idx, _ = token_maps\n",
    "    start_token_id = token_to_idx['<START>']\n",
    "    stop_token_id = token_to_idx['<END>']\n",
    "    pad_token_id = token_to_idx['<PAD>']\n",
    "    \n",
    "    top_k = 50\n",
    "\n",
    "    generated_seqs = torch.tensor([[start_token_id]] * batch_size, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1): # -1 for the start token\n",
    "            logits = generator(generated_seqs, props_to_use)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            \n",
    "            v, _ = torch.topk(last_logits, top_k)\n",
    "            last_logits[last_logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "            probs = F.softmax(last_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated_seqs = torch.cat([generated_seqs, next_token], dim=1)\n",
    "            \n",
    "            # Stop if all sequences have hit the <END> token\n",
    "            if (next_token.squeeze() == stop_token_id).all():\n",
    "                break\n",
    "    \n",
    "    # --- CRITICAL: Pad all sequences to max_len ---\n",
    "    B, L = generated_seqs.shape\n",
    "    if L < max_len:\n",
    "        pads = torch.full((B, max_len - L), pad_token_id, dtype=torch.long, device=device)\n",
    "        generated_seqs = torch.cat([generated_seqs, pads], dim=1)\n",
    "    \n",
    "    return generated_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48440053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading real encoded sequences from ../data/processed_5l/train_encoded.pt...\n",
      "✅ Loaded encoded sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Generator from epoch 50\n",
      "✅ Setup complete. Starting discriminator training...\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Paths and Hyperparameters ---\n",
    "CHECKPOINT_DIR = \"../results/models_5l/\"\n",
    "TRAIN_ENCODED_PATH = \"../data/processed_5l/train_encoded.pt\"\n",
    "TRAIN_PROPERTIES_CSV = \"../data/processed_5l/train_properties.csv\"\n",
    "GEN_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"u&c_generator_epoch_50.pt\") \n",
    "\n",
    "# Model Hyperparameters (Must match your Generator)\n",
    "VOCAB_SIZE = 70 \n",
    "PROP_DIM = 5\n",
    "D_MODEL = 256\n",
    "N_HEAD = 8\n",
    "NUM_LAYERS = 4\n",
    "MAX_LEN = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training Hyperparameters\n",
    "TOTAL_EPOCHS = 10  # Pre-training the discriminator is usually fast (5-10 epochs)\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 64  # Using the same batch size as your last G-run\n",
    "p_uncond = 0.1   # Must match the Generator's training!\n",
    "\n",
    "# --- 1. Data and Device Setup ---\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dataset = MoleculeDataset(encoded_path=TRAIN_ENCODED_PATH, properties_csv=TRAIN_PROPERTIES_CSV)\n",
    "dataset.encoded_sequences = dataset.encoded_sequences.long() \n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "token_to_idx, idx_to_token = get_token_maps()\n",
    "token_maps = (token_to_idx, idx_to_token)\n",
    "\n",
    "# --- 2. Load Pre-trained Generator (for generating fake samples) ---\n",
    "generator = Generator(\n",
    "    vocab_size=VOCAB_SIZE, prop_dim=PROP_DIM, d_model=D_MODEL, nhead=N_HEAD, \n",
    "    num_layers=NUM_LAYERS, max_len=MAX_LEN, dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "gen_checkpoint = torch.load(GEN_CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "generator.load_state_dict(gen_checkpoint['model_state_dict'])\n",
    "generator.eval() # Set to eval mode, we are only using it for inference\n",
    "print(f\" Loaded Generator from epoch {gen_checkpoint['epoch']}\")\n",
    "\n",
    "# --- 3. Initialize Discriminator and Optimizer ---\n",
    "discriminator = Discriminator(\n",
    "    vocab_size=VOCAB_SIZE, prop_dim=PROP_DIM, d_model=D_MODEL, nhead=N_HEAD, \n",
    "    num_layers=NUM_LAYERS, max_len=MAX_LEN, dropout=DROPOUT\n",
    ").to(device) \n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss() # Standard for GANs\n",
    "start_epoch = 0\n",
    "\n",
    "print(\" Setup complete. Starting discriminator training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0b4c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 3\n",
    "TOTAL_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e13ba0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disc Epoch 1/3: 100%|██████████| 4359/4359 [2:09:19<00:00,  1.78s/it, avg_loss=0.0280, fake_loss=0.0109, real_loss=0.0102]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint saved to ../results/models_5l/discriminator_epoch_1.pt ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disc Epoch 2/3:   3%|▎         | 141/4359 [04:01<2:00:19,  1.71s/it, avg_loss=0.0267, fake_loss=0.0300, real_loss=0.0257]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     31\u001b[39m loss_real.backward() \u001b[38;5;66;03m# Calculate gradients\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# --- B. Train on Fake Samples (Target Label = 0.0) ---\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Generate full, autoregressive fake samples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m fake_seqs = \u001b[43mgenerate_fake_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprops_to_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_LEN\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Use .detach() to stop gradients from flowing back into the generator\u001b[39;00m\n\u001b[32m     41\u001b[39m fake_logits = discriminator(fake_seqs.detach(), props_to_use)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 136\u001b[39m, in \u001b[36mgenerate_fake_samples\u001b[39m\u001b[34m(generator, props_to_use, batch_size, device, token_maps, max_len)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_len - \u001b[32m1\u001b[39m): \u001b[38;5;66;03m# -1 for the start token\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m         logits = \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprops_to_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m         last_logits = logits[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m    139\u001b[39m         v, _ = torch.topk(last_logits, top_k)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mGenerator.forward\u001b[39m\u001b[34m(self, src, props)\u001b[39m\n\u001b[32m     28\u001b[39m x = x.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Transformer expects [L, B, D]\u001b[39;00m\n\u001b[32m     30\u001b[39m tgt_mask = nn.Transformer.generate_square_subsequent_mask(L).to(src.device)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m out = out.transpose(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m) \n\u001b[32m     34\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.fc_out(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:511\u001b[39m, in \u001b[36mTransformerEncoder.forward\u001b[39m\u001b[34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    508\u001b[39m is_causal = _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[32m    519\u001b[39m     output = output.to_padded_tensor(\u001b[32m0.0\u001b[39m, src.size())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:906\u001b[39m, in \u001b[36mTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    902\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m    903\u001b[39m         x\n\u001b[32m    904\u001b[39m         + \u001b[38;5;28mself\u001b[39m._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n\u001b[32m    905\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:931\u001b[39m, in \u001b[36mTransformerEncoderLayer._ff_block\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\Desktop\\bio-info data\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 4. Training Loop ---\n",
    "for epoch in range(start_epoch, TOTAL_EPOCHS):\n",
    "    discriminator.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    batch_iterator = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        desc=f\"Disc Epoch {epoch+1}/{TOTAL_EPOCHS}\", \n",
    "        total=len(dataloader)\n",
    "    )\n",
    "\n",
    "    for i, (real_seqs, real_props) in batch_iterator:\n",
    "        real_seqs, real_props = real_seqs.to(device), real_props.to(device)\n",
    "        batch_size = real_seqs.size(0)\n",
    "        \n",
    "        # --- Handle incomplete last batch ---\n",
    "        if batch_size != BATCH_SIZE:\n",
    "            continue # Skip batches that aren't full\n",
    "            \n",
    "        # --- Conditional Dropping ---\n",
    "        # Teach the Discriminator to also be an unconditional critic\n",
    "        props_to_use = real_props\n",
    "        if torch.rand(1).item() < p_uncond:\n",
    "            props_to_use = torch.zeros_like(real_props)\n",
    "        \n",
    "        # --- A. Train on Real Samples (Target Label = 1.0) ---\n",
    "        discriminator.zero_grad()\n",
    "        real_logits = discriminator(real_seqs, props_to_use)\n",
    "        real_labels = torch.ones(batch_size, device=device) \n",
    "        loss_real = criterion(real_logits, real_labels)\n",
    "        loss_real.backward() # Calculate gradients\n",
    "\n",
    "        # --- B. Train on Fake Samples (Target Label = 0.0) ---\n",
    "        \n",
    "        # Generate full, autoregressive fake samples\n",
    "        fake_seqs = generate_fake_samples(\n",
    "            generator, props_to_use, batch_size, device, token_maps, max_len=MAX_LEN\n",
    "        )\n",
    "        \n",
    "        # Use .detach() to stop gradients from flowing back into the generator\n",
    "        fake_logits = discriminator(fake_seqs.detach(), props_to_use)\n",
    "        fake_labels = torch.zeros(batch_size, device=device)\n",
    "        loss_fake = criterion(fake_logits, fake_labels)\n",
    "        loss_fake.backward() # Calculate gradients\n",
    "        \n",
    "        # --- C. Update Discriminator ---\n",
    "        # The optimizer steps *after* both .backward() calls\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        loss_D = loss_real + loss_fake\n",
    "        total_loss += loss_D.item()\n",
    "        \n",
    "        running_avg_loss = total_loss / (i + 1)\n",
    "        batch_iterator.set_postfix(\n",
    "            avg_loss=f\"{running_avg_loss:.4f}\", \n",
    "            real_loss=f\"{loss_real.item():.4f}\", \n",
    "            fake_loss=f\"{loss_fake.item():.4f}\"\n",
    "        )\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader) \n",
    "    print()\n",
    "\n",
    "    # --- 5. Save Checkpoint ---\n",
    "    checkpoint_path = f\"{CHECKPOINT_DIR}discriminator_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "        'loss': avg_loss\n",
    "    }, checkpoint_path)\n",
    "    print(f\"--- Checkpoint saved to {checkpoint_path} ---\")\n",
    "\n",
    "print(\"✅ Discriminator Pre-training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d9dfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Disable gradients for this entire function\n",
    "def validate_discriminator(discriminator, generator, val_dataloader, device, token_maps, p_uncond=0.1):\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "    \n",
    "    total_real_correct = 0\n",
    "    total_fake_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    batch_iterator = tqdm(val_dataloader, desc=\"Validating Discriminator\")\n",
    "\n",
    "    for real_seqs, real_props in batch_iterator:\n",
    "        real_seqs, real_props = real_seqs.to(device), real_props.to(device)\n",
    "        batch_size = real_seqs.size(0)\n",
    "        \n",
    "        # --- Handle incomplete last batch ---\n",
    "        if batch_size != val_dataloader.batch_size:\n",
    "            continue\n",
    "            \n",
    "        total_samples += batch_size\n",
    "\n",
    "        # --- Decide on properties (conditional or unconditional) ---\n",
    "        props_to_use = real_props\n",
    "        if torch.rand(1).item() < p_uncond:\n",
    "            props_to_use = torch.zeros_like(real_props)\n",
    "\n",
    "        # --- 1. Test on REAL data ---\n",
    "        # We expect the logit to be positive (> 0)\n",
    "        real_logits = discriminator(real_seqs, props_to_use)\n",
    "        total_real_correct += (real_logits > 0).sum().item()\n",
    "\n",
    "        # --- 2. Test on FAKE data ---\n",
    "        fake_seqs = generate_fake_samples(\n",
    "            generator, props_to_use, batch_size, device, token_maps\n",
    "        )\n",
    "        \n",
    "        # We expect the logit to be negative (< 0)\n",
    "        fake_logits = discriminator(fake_seqs, props_to_use)\n",
    "        total_fake_correct += (fake_logits < 0).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        real_acc = (total_real_correct / total_samples) * 100\n",
    "        fake_acc = (total_fake_correct / total_samples) * 100\n",
    "        batch_iterator.set_postfix(\n",
    "            real_acc=f\"{real_acc:.2f}%\", \n",
    "            fake_acc=f\"{fake_acc:.2f}%\"\n",
    "        )\n",
    "    \n",
    "    print(\"\\n--- Validation Results ---\")\n",
    "    final_real_acc = (total_real_correct / total_samples) * 100\n",
    "    final_fake_acc = (total_fake_correct / total_samples) * 100\n",
    "    final_total_acc = ((total_real_correct + total_fake_correct) / (total_samples * 2)) * 100\n",
    "    \n",
    "    print(f\" Real Accuracy: {final_real_acc:.2f}%\")\n",
    "    print(f\" Fake Accuracy: {final_fake_acc:.2f}%\")\n",
    "    print(f\" Total Accuracy: {final_total_acc:.2f}%\")\n",
    "    print(\"\\n(This is the discriminator's performance on unseen data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd0457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading real encoded sequences from ../data/processed_5l/val_encoded.pt...\n",
      "✅ Loaded encoded sequences.\n",
      " Loaded validation dataset from ../data/processed_5l/val_properties.csv\n",
      "Loading final discriminator from: ../results/models_5l/discriminator_epoch_1.pt\n",
      " Loaded Discriminator from epoch 1 for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Discriminator: 100%|██████████| 243/243 [06:41<00:00,  1.65s/it, fake_acc=98.20%, real_acc=99.16%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Validation Results ---\n",
      " Real Accuracy: 99.16%\n",
      " Fake Accuracy: 98.20%\n",
      " Total Accuracy: 98.68%\n",
      "\n",
      "(This is the discriminator's performance on unseen data)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Define Validation Data Paths ---\n",
    "VAL_ENCODED_PATH = \"../data/processed_5l/val_encoded.pt\"\n",
    "VAL_PROPERTIES_CSV = \"../data/processed_5l/val_properties.csv\"\n",
    "\n",
    "# --- 2. Load the Validation Dataset ---\n",
    "try:\n",
    "    val_dataset = MoleculeDataset(\n",
    "        encoded_path=VAL_ENCODED_PATH, \n",
    "        properties_csv=VAL_PROPERTIES_CSV\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, # Use BATCH_SIZE defined in the training cell\n",
    "        shuffle=False\n",
    "    )\n",
    "    print(f\" Loaded validation dataset from {VAL_PROPERTIES_CSV}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" ERROR: Validation data not found. Skipping validation.\")\n",
    "    print(f\"    - Searched for: {VAL_ENCODED_PATH}\")\n",
    "    print(f\"    - Searched for: {VAL_PROPERTIES_CSV}\")\n",
    "    val_dataloader = None\n",
    "\n",
    "# --- 3. Load the Final Discriminator Checkpoint ---\n",
    "final_disc_epoch = TOTAL_EPOCHS \n",
    "DISC_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"discriminator_epoch_1.pt\")\n",
    "\n",
    "if os.path.exists(DISC_CHECKPOINT_PATH) and val_dataloader:\n",
    "    print(f\"Loading final discriminator from: {DISC_CHECKPOINT_PATH}\")\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    disc_checkpoint = torch.load(DISC_CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "    \n",
    "    # Load the weights into the 'discriminator' model object from the training cell\n",
    "    discriminator.load_state_dict(disc_checkpoint['model_state_dict'])\n",
    "    print(f\" Loaded Discriminator from epoch {disc_checkpoint['epoch']} for validation.\")\n",
    "\n",
    "    # --- 4. Run Validation ---\n",
    "    # The 'generator' object is already loaded and in eval mode\n",
    "    # The 'discriminator' object is now loaded\n",
    "    # The 'val_dataloader', 'device', 'token_maps', and 'p_uncond' are all defined\n",
    "    \n",
    "    validate_discriminator(\n",
    "        discriminator=discriminator, \n",
    "        generator=generator, \n",
    "        val_dataloader=val_dataloader, \n",
    "        device=device, \n",
    "        token_maps=token_maps, \n",
    "        p_uncond=p_uncond\n",
    "    )\n",
    "else:\n",
    "    if not val_dataloader:\n",
    "        print(\"Skipping validation because validation data was not found.\")\n",
    "    else:\n",
    "        print(f\" ERROR: Final discriminator checkpoint not found at {DISC_CHECKPOINT_PATH}\")\n",
    "        print(\"Skipping validation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
